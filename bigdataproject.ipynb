{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. read in the dataset\n",
    "data_df = spark.read\\\n",
    "  .format('csv')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('s3://bigdataproject233/stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+--------------------+-------+----+-----------+------------+\n",
      "|     id|          by|score|        time|             time_ts|               title|                 url|                text|deleted|dead|descendants|      author|\n",
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+--------------------+-------+----+-----------+------------+\n",
      "|6940813|   sarath237|  0.0|1387536270.0|2013-12-20 10:44:...| Sheryl Brindo Ho...|http://www.youtub...| Sheryl Brindo Ho...|   null|True|       null|   sarath237|\n",
      "|6991401|123123321321|  0.0|1388508751.0|2013-12-31 16:52:...|Are you people al...|                null|They&#x27;re pret...|   null|True|       null|123123321321|\n",
      "|1531556|         ssn|  0.0|1279617234.0|2010-07-20 09:13:...|New UI for Google...|http://googlesyst...|Again following o...|   null|null|        0.0|         ssn|\n",
      "|5012398|        hoju|  0.0|1357387877.0|2013-01-05 12:11:...|Historic website ...|http://webscrapin...|Python script to ...|   null|null|        0.0|        hoju|\n",
      "|7214182|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|1187303|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|1318494|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3700400|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|1384487|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3530428|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3538851|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|4167497|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3789206|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|4072660|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|2979883|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3300974|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|2305819|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3833628|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|2606988|       ramis|  0.0|1306931771.0|2011-06-01 12:36:...|Kidney Transplant...|http://dhshahzad....|I started indepen...|   null|True|        0.0|       ramis|\n",
      "|4542754|       tette|  0.0|1348045069.0|2012-09-19 08:57:...|Bouncing Marble f...|http://www.window...|Bouncing marble i...|   null|null|        0.0|       tette|\n",
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+--------------------+-------+----+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- by: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- time_ts: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- deleted: string (nullable = true)\n",
      " |-- dead: string (nullable = true)\n",
      " |-- descendants: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2069464"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df_total = data_df.select(data_df['id'].cast(IntegerType()),\n",
    "                     data_df['score'].cast(IntegerType()),\n",
    "                     #data_df[\"time\"],\n",
    "                     data_df['time_ts'].cast('timestamp'),\n",
    "                     data_df['title'],\n",
    "                     #data_df['type'],\n",
    "                     data_df['url'],\n",
    "                     data_df['text'],\n",
    "                     #data_df['parent'].cast(IntegerType()),\n",
    "                     #data_df['deleted'],\n",
    "                     #data_df['dead'],\n",
    "                     data_df['descendants'].cast(IntegerType()),\n",
    "                     #data_df['id'].cast(IntegerType()),\n",
    "                     data_df['author']\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- time_ts: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- descendants: integer (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_total.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------------+--------------------+--------------------+--------------------+-----------+------------+\n",
      "|     id|score|            time_ts|               title|                 url|                text|descendants|      author|\n",
      "+-------+-----+-------------------+--------------------+--------------------+--------------------+-----------+------------+\n",
      "|6940813|    0|2013-12-20 10:44:30| Sheryl Brindo Ho...|http://www.youtub...| Sheryl Brindo Ho...|       null|   sarath237|\n",
      "|6991401|    0|2013-12-31 16:52:31|Are you people al...|                null|They&#x27;re pret...|       null|123123321321|\n",
      "|1531556|    0|2010-07-20 09:13:54|New UI for Google...|http://googlesyst...|Again following o...|          0|         ssn|\n",
      "|5012398|    0|2013-01-05 12:11:17|Historic website ...|http://webscrapin...|Python script to ...|          0|        hoju|\n",
      "|7214182|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|1187303|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|1318494|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|3700400|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|1384487|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|3530428|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "+-------+-----+-------------------+--------------------+--------------------+--------------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_total.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_total.createOrReplaceTempView('story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Story data \n",
    "#story = spark.sql('SELECT * FROM data WHERE type == \"story\"')\n",
    "#story.createOrReplaceTempView('story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1959840"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.select(\"id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759584"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.select(\"title\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1925929"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.select(\"time_ts\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df = spark.sql('select * from story where title is not NULL and author is not NULL and score is not NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1807837"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df.createOrReplaceTempView('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1807837"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is score as NULL in the dataset where title is not NULL\n",
    "#spark.sql('select score from title where score is not NULL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#待完成\n",
    "#url_df = spark.sql(\"select score, by as author, title, regexp_extract(url, '/(\\w+):\\/\\/([^/:]+)(:\\d*)?([^# ]*)/', 1) as url from title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|url|\n",
      "+---+\n",
      "|   |\n",
      "|   |\n",
      "+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#url_df.select('url').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(score)=0, max(score)=75248, avg(score)=10.447452950680841)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score information\n",
    "spark.sql(\"select min(score),max(score),avg(score) from title\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author/user information \n",
    "# decending by total_score \n",
    "sqlStatement = \"\"\"SELECT SUM(score) as total_score,author,\n",
    "SUM(descendants) as total_decendants,\n",
    "count(id) as total_stories\n",
    "FROM title\n",
    "GROUP BY author\n",
    "ORDER BY total_score DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "user_df = spark.sql(sqlStatement)\n",
    "user_df.createOrReplaceTempView(\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+-------------+\n",
      "|total_score|     author|total_decendants|total_stories|\n",
      "+-----------+-----------+----------------+-------------+\n",
      "|      76851|ColinWright|           30700|         3726|\n",
      "|      75248|          0|         1732237|            0|\n",
      "|      64156| shawndumas|           25926|         6585|\n",
      "|      59958|    llambda|           20324|         2595|\n",
      "|      56798|      fogus|           21136|         2412|\n",
      "|      53452|      danso|           22792|         2610|\n",
      "|      52701|       cwan|           24067|         7059|\n",
      "|      51836|        luu|           19713|         2265|\n",
      "|      49003|  ssclafani|           24026|         1324|\n",
      "|      41601|      evo_9|           16215|         5635|\n",
      "+-----------+-----------+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author/user information \n",
    "# decending by total_stories \n",
    "sqlStatement = \"\"\"SELECT SUM(score) as total_score, author,\n",
    "SUM(descendants) as total_decendants,\n",
    "count(id) as total_stories\n",
    "FROM title\n",
    "GROUP BY author\n",
    "ORDER BY total_stories DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "user_df2 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+-------------+\n",
      "|total_score|     author|total_decendants|total_stories|\n",
      "+-----------+-----------+----------------+-------------+\n",
      "|      52701|       cwan|           24067|         7059|\n",
      "|      64156| shawndumas|           25926|         6585|\n",
      "|      41601|      evo_9|           16215|         5635|\n",
      "|      29477|      nickb|           11804|         4303|\n",
      "|      26431|   iProject|           11759|         4262|\n",
      "|      28454|   bootload|           11351|         4158|\n",
      "|      29598|     edw519|           13726|         3823|\n",
      "|      76851|ColinWright|           30700|         3726|\n",
      "|      29806|     nreece|           12554|         3713|\n",
      "|      36511| tokenadult|           20190|         3634|\n",
      "+-----------+-----------+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author/user information \n",
    "# decending by avg_score \n",
    "sqlStatement = \"\"\"SELECT SUM(score) as total_score, author,\n",
    "SUM(descendants) as total_decendants, SUM(score)/count(id) as avg_score,\n",
    "count(id) as total_stories\n",
    "FROM title\n",
    "GROUP BY author\n",
    "ORDER BY avg_score DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "user_df3 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------------+---------+-------------+\n",
      "|total_score|         author|total_decendants|avg_score|total_stories|\n",
      "+-----------+---------------+----------------+---------+-------------+\n",
      "|       1543|    realfuncode|             526|   1543.0|            1|\n",
      "|       2905|     frederfred|             412|   1452.5|            2|\n",
      "|       1344| themanthatfell|             407|   1344.0|            1|\n",
      "|       1282|          rcina|             249|   1282.0|            1|\n",
      "|       1257|         kvargs|             558|   1257.0|            1|\n",
      "|       1248|        mmebane|             267|   1248.0|            1|\n",
      "|       1227|FlemishBeeCycle|             444|   1227.0|            1|\n",
      "|       1172|     hannahmitt|             136|   1172.0|            1|\n",
      "|       1125|  afraidofadria|             985|   1125.0|            1|\n",
      "|       4354|   patricktomas|             385|   1088.5|            4|\n",
      "+-----------+---------------+----------------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is bias on YC\n",
    "sqlStatement = \"\"\"\n",
    "SELECT score, title,id\n",
    "FROM `title`\n",
    "WHERE title like \"%Y Combinator%\" or title like \"%YCombinator%\" or title like \"%ycombinator%\" or title like \"%y combinator%\"\n",
    "ORDER BY score  DESC\n",
    "\"\"\"\n",
    "YC_df = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+\n",
      "|score|               title|      id|\n",
      "+-----+--------------------+--------+\n",
      "| 1065|Y Combinator is f...| 5059806|\n",
      "|  841|Y Combinator has ...| 8033322|\n",
      "|  705|Meet Watsi, Y Com...| 5117385|\n",
      "|  687|New: Apply to Y C...| 3700712|\n",
      "|  634|I Am Sam Altman, ...| 9238839|\n",
      "|  589|How I Got Kicked ...| 2208155|\n",
      "|  550|Benefits matter, ...| 5409273|\n",
      "|  549|What Happens At Y...| 1733236|\n",
      "|  542|How Y Combinator ...| 3711008|\n",
      "|  506|Y Combinator Numbers| 2608440|\n",
      "|  432|New Y Combinator ...| 7972138|\n",
      "|  425|Yuri Milner, SV A...| 2154706|\n",
      "|  380|Y Combinator And ...| 8178450|\n",
      "|  379|How I Crashed and...| 8867335|\n",
      "|  368|Early Photos of Y...| 2942958|\n",
      "|  366|Investment Firm Y...| 3492711|\n",
      "|  344|Y Combinator anno...| 1898432|\n",
      "|  342|Offer HN now at n...| 1840060|\n",
      "|  337|A note I sent to ...| 4273460|\n",
      "|  336|I am Sam Altman, ...|10360911|\n",
      "+-----+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YC_df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories of major companies over the year\n",
    "# apple\n",
    "sqlStatement = \"\"\"\n",
    "SELECT count(*) as total_stories, SUM(score) as total_score,\n",
    "year(time_ts) as year\n",
    "FROM title\n",
    "WHERE title like \"%apple%\" or title like \"%APPLE\" or title like \"%Apple\"\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "Comp_df1 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----+\n",
      "|total_stories|total_score|year|\n",
      "+-------------+-----------+----+\n",
      "|            1|          7|2006|\n",
      "|           28|        109|2007|\n",
      "|           43|        324|2008|\n",
      "|           86|        780|2009|\n",
      "|          194|       1964|2010|\n",
      "|          348|       5316|2011|\n",
      "|          382|       4883|2012|\n",
      "|          280|       2345|2013|\n",
      "|          203|       2302|2014|\n",
      "|          168|       3672|2015|\n",
      "+-------------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Comp_df1.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories of major companies over the year\n",
    "# g\n",
    "sqlStatement = \"\"\"\n",
    "SELECT count(*) as total_stories, SUM(score) as total_score,\n",
    "year(time_ts) as year\n",
    "FROM title\n",
    "WHERE title like \"%google%\" or title like \"%GOOGLE\" or title like \"%Google\"\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "Comp_df2 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----+\n",
      "|total_stories|total_score|year|\n",
      "+-------------+-----------+----+\n",
      "|            1|          1|2006|\n",
      "|          102|        514|2007|\n",
      "|          190|       1192|2008|\n",
      "|          428|       3376|2009|\n",
      "|          553|       6938|2010|\n",
      "|          506|       9489|2011|\n",
      "|          461|       7592|2012|\n",
      "|          478|      10844|2013|\n",
      "|          432|       7835|2014|\n",
      "|          369|       9247|2015|\n",
      "+-------------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Comp_df2.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories of major companies over the year\n",
    "# apple\n",
    "sqlStatement = \"\"\"\n",
    "SELECT count(*) as total_stories, SUM(score) as total_score,\n",
    "year(time_ts) as year\n",
    "FROM title\n",
    "WHERE title like \"%uber%\" or title like \"%UBER\" or title like \"%Uber\"\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "Comp_df3 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comp_df3.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
