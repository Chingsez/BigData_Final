{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. STORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset stories\n",
    "data_df = spark.read\\\n",
    "  .format('csv')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('s3://bigdataproject233/stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+--------------------+-------+----+-----------+------------+\n",
      "|     id|          by|score|        time|             time_ts|               title|                 url|                text|deleted|dead|descendants|      author|\n",
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+--------------------+-------+----+-----------+------------+\n",
      "|6940813|   sarath237|  0.0|1387536270.0|2013-12-20 10:44:...| Sheryl Brindo Ho...|http://www.youtub...| Sheryl Brindo Ho...|   null|True|       null|   sarath237|\n",
      "|6991401|123123321321|  0.0|1388508751.0|2013-12-31 16:52:...|Are you people al...|                null|They&#x27;re pret...|   null|True|       null|123123321321|\n",
      "|1531556|         ssn|  0.0|1279617234.0|2010-07-20 09:13:...|New UI for Google...|http://googlesyst...|Again following o...|   null|null|        0.0|         ssn|\n",
      "|5012398|        hoju|  0.0|1357387877.0|2013-01-05 12:11:...|Historic website ...|http://webscrapin...|Python script to ...|   null|null|        0.0|        hoju|\n",
      "|7214182|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|1187303|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|1318494|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3700400|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|1384487|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3530428|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3538851|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|4167497|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3789206|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|4072660|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|2979883|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3300974|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|2305819|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|3833628|       kogir|  0.0|1401561740.0|2014-05-31 18:42:...|         Placeholder|                null|       Mind the gap.|   null|null|        0.0|       kogir|\n",
      "|2606988|       ramis|  0.0|1306931771.0|2011-06-01 12:36:...|Kidney Transplant...|http://dhshahzad....|I started indepen...|   null|True|        0.0|       ramis|\n",
      "|4542754|       tette|  0.0|1348045069.0|2012-09-19 08:57:...|Bouncing Marble f...|http://www.window...|Bouncing marble i...|   null|null|        0.0|       tette|\n",
      "+-------+------------+-----+------------+--------------------+--------------------+--------------------+--------------------+-------+----+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- by: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- time_ts: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- deleted: string (nullable = true)\n",
      " |-- dead: string (nullable = true)\n",
      " |-- descendants: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2069464"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "story_df = data_df.select(data_df['id'].cast(IntegerType()),\n",
    "                     data_df['score'].cast(IntegerType()),\n",
    "                     #data_df[\"time\"],\n",
    "                     data_df['time_ts'].cast('timestamp'),\n",
    "                     data_df['title'],\n",
    "                     #data_df['type'],\n",
    "                     data_df['url'],\n",
    "                     data_df['text'],\n",
    "                     #data_df['parent'].cast(IntegerType()),\n",
    "                     #data_df['deleted'],\n",
    "                     #data_df['dead'],\n",
    "                     data_df['descendants'].cast(IntegerType()),\n",
    "                     #data_df['id'].cast(IntegerType()),\n",
    "                     data_df['author']\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- time_ts: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- descendants: integer (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------------+--------------------+--------------------+--------------------+-----------+------------+\n",
      "|     id|score|            time_ts|               title|                 url|                text|descendants|      author|\n",
      "+-------+-----+-------------------+--------------------+--------------------+--------------------+-----------+------------+\n",
      "|6940813|    0|2013-12-20 10:44:30| Sheryl Brindo Ho...|http://www.youtub...| Sheryl Brindo Ho...|       null|   sarath237|\n",
      "|6991401|    0|2013-12-31 16:52:31|Are you people al...|                null|They&#x27;re pret...|       null|123123321321|\n",
      "|1531556|    0|2010-07-20 09:13:54|New UI for Google...|http://googlesyst...|Again following o...|          0|         ssn|\n",
      "|5012398|    0|2013-01-05 12:11:17|Historic website ...|http://webscrapin...|Python script to ...|          0|        hoju|\n",
      "|7214182|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|1187303|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|1318494|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|3700400|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|1384487|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "|3530428|    0|2014-05-31 18:42:20|         Placeholder|                null|       Mind the gap.|          0|       kogir|\n",
      "+-------+-----+-------------------+--------------------+--------------------+--------------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp table\n",
    "story_df.createOrReplaceTempView('ini_story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1959840"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_df.select(\"id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759584"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_df.select(\"title\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1925929"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_df.select(\"time_ts\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_story_df = spark.sql('select * from ini_story where time_ts is not NULL and title is not NULL and author is not NULL and score is not NULL')\n",
    "clean_story_df.createOrReplaceTempView('story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1807831"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_story_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are the stories coming from? or what are the most popular sources?\n",
    "# 写的不对\n",
    "sqlStatement = \"\"\"\n",
    "SELECT  COUNT(id) AS story_num, \n",
    "regexp_extract(url, '^http[s]?://(([a-zA-Z0-9\\._-]+\\.[a-zA-Z]{2,6})|\n",
    "([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))\n",
    "(:[0-9]{1,4})*(/[a-zA-Z0-9\\&%_\\./-~-]*)?', 1) as url \n",
    "FROM story \n",
    "WHERE url is not NULL\n",
    "GROUP BY url\n",
    "ORDER BY story_num\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "url_df = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|url|\n",
      "+---+\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "|   |\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_df.select('url').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# story distribution in terms of time\n",
    "# What is the trend of story numbers from 2006-2016 in Hacker News?\n",
    "\n",
    "sqlStatement = \"\"\"\n",
    "SELECT COUNT(id) AS story_num, \n",
    "year(time_ts) as year\n",
    "FROM story\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "story_time_df1 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|story_num|year|\n",
      "+---------+----+\n",
      "|       50|2006|\n",
      "|    21948|2007|\n",
      "|    68877|2008|\n",
      "|   109468|2009|\n",
      "|   175246|2010|\n",
      "|   285433|2011|\n",
      "|   305361|2012|\n",
      "|   303813|2013|\n",
      "|   287287|2014|\n",
      "|   250348|2015|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story_time_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# story distribution in terms of time\n",
    "# What is the most popular/active time during a day in pubulishing a story?\n",
    "\n",
    "sqlStatement = \"\"\"\n",
    "SELECT COUNT(id) AS story_num, \n",
    "hour(time_ts) as hour\n",
    "FROM story\n",
    "GROUP BY hour\n",
    "ORDER BY story_num DESC\n",
    "\"\"\"\n",
    "story_time_df2 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|story_num|hour|\n",
      "+---------+----+\n",
      "|   118330|  16|\n",
      "|   116519|  17|\n",
      "|   114846|  15|\n",
      "|   111187|  18|\n",
      "|   107289|  14|\n",
      "|   101710|  19|\n",
      "|    95143|  20|\n",
      "|    92281|  13|\n",
      "|    87559|  21|\n",
      "|    75879|  12|\n",
      "|    75403|  22|\n",
      "|    64806|  23|\n",
      "|    62944|  11|\n",
      "|    58054|   0|\n",
      "|    55560|  10|\n",
      "|    54969|   1|\n",
      "|    54117|   9|\n",
      "|    53419|   2|\n",
      "|    52016|   3|\n",
      "|    51697|   6|\n",
      "|    51264|   8|\n",
      "|    51254|   4|\n",
      "|    50800|   7|\n",
      "|    50785|   5|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story_time_df2.show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(score)=0, max(score)=75248, avg(score)=10.447452950680841)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score information\n",
    "# what is the average score of those story?\n",
    "spark.sql(\"select min(score),max(score),avg(score) from story\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author/user information \n",
    "# decending by total_score \n",
    "sqlStatement = \"\"\"SELECT SUM(score) as total_score,author,\n",
    "SUM(descendants) as total_decendants,\n",
    "count(id) as total_stories\n",
    "FROM story\n",
    "GROUP BY author\n",
    "ORDER BY total_score DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "user_df = spark.sql(sqlStatement)\n",
    "#user_df.createOrReplaceTempView(\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+-------------+\n",
      "|total_score|     author|total_decendants|total_stories|\n",
      "+-----------+-----------+----------------+-------------+\n",
      "|      76851|ColinWright|           30700|         3726|\n",
      "|      75248|          0|         1732237|            0|\n",
      "|      64156| shawndumas|           25926|         6585|\n",
      "|      59958|    llambda|           20324|         2595|\n",
      "|      56798|      fogus|           21136|         2412|\n",
      "|      53452|      danso|           22792|         2610|\n",
      "|      52701|       cwan|           24067|         7059|\n",
      "|      51836|        luu|           19713|         2265|\n",
      "|      49003|  ssclafani|           24026|         1324|\n",
      "|      41601|      evo_9|           16215|         5635|\n",
      "+-----------+-----------+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author/user information \n",
    "# decending by total_stories \n",
    "sqlStatement = \"\"\"SELECT SUM(score) as total_score, author,\n",
    "SUM(descendants) as total_decendants,\n",
    "count(id) as total_stories\n",
    "FROM story\n",
    "GROUP BY author\n",
    "ORDER BY total_stories DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "user_df2 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+-------------+\n",
      "|total_score|     author|total_decendants|total_stories|\n",
      "+-----------+-----------+----------------+-------------+\n",
      "|      52701|       cwan|           24067|         7059|\n",
      "|      64156| shawndumas|           25926|         6585|\n",
      "|      41601|      evo_9|           16215|         5635|\n",
      "|      29477|      nickb|           11804|         4303|\n",
      "|      26431|   iProject|           11759|         4262|\n",
      "|      28454|   bootload|           11351|         4158|\n",
      "|      29598|     edw519|           13726|         3823|\n",
      "|      76851|ColinWright|           30700|         3726|\n",
      "|      29806|     nreece|           12554|         3713|\n",
      "|      36511| tokenadult|           20190|         3634|\n",
      "+-----------+-----------+----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author/user information \n",
    "# decending by avg_score \n",
    "sqlStatement = \"\"\"SELECT SUM(score) as total_score, author,\n",
    "SUM(descendants) as total_decendants, SUM(score)/count(id) as avg_score,\n",
    "count(id) as total_stories\n",
    "FROM story\n",
    "GROUP BY author\n",
    "ORDER BY avg_score DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "user_df3 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------------+---------+-------------+\n",
      "|total_score|         author|total_decendants|avg_score|total_stories|\n",
      "+-----------+---------------+----------------+---------+-------------+\n",
      "|       1543|    realfuncode|             526|   1543.0|            1|\n",
      "|       2905|     frederfred|             412|   1452.5|            2|\n",
      "|       1344| themanthatfell|             407|   1344.0|            1|\n",
      "|       1282|          rcina|             249|   1282.0|            1|\n",
      "|       1257|         kvargs|             558|   1257.0|            1|\n",
      "|       1248|        mmebane|             267|   1248.0|            1|\n",
      "|       1227|FlemishBeeCycle|             444|   1227.0|            1|\n",
      "|       1172|     hannahmitt|             136|   1172.0|            1|\n",
      "|       1125|  afraidofadria|             985|   1125.0|            1|\n",
      "|       4354|   patricktomas|             385|   1088.5|            4|\n",
      "+-----------+---------------+----------------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is bias on YC\n",
    "sqlStatement = \"\"\"\n",
    "SELECT score, title,id\n",
    "FROM `story`\n",
    "WHERE title like \"%Y Combinator%\" or title like \"%YCombinator%\" or title like \"%ycombinator%\" or title like \"%y combinator%\"\n",
    "ORDER BY score  DESC\n",
    "\"\"\"\n",
    "YC_df = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+\n",
      "|score|               title|      id|\n",
      "+-----+--------------------+--------+\n",
      "| 1065|Y Combinator is f...| 5059806|\n",
      "|  841|Y Combinator has ...| 8033322|\n",
      "|  705|Meet Watsi, Y Com...| 5117385|\n",
      "|  687|New: Apply to Y C...| 3700712|\n",
      "|  634|I Am Sam Altman, ...| 9238839|\n",
      "|  589|How I Got Kicked ...| 2208155|\n",
      "|  550|Benefits matter, ...| 5409273|\n",
      "|  549|What Happens At Y...| 1733236|\n",
      "|  542|How Y Combinator ...| 3711008|\n",
      "|  506|Y Combinator Numbers| 2608440|\n",
      "|  432|New Y Combinator ...| 7972138|\n",
      "|  425|Yuri Milner, SV A...| 2154706|\n",
      "|  380|Y Combinator And ...| 8178450|\n",
      "|  379|How I Crashed and...| 8867335|\n",
      "|  368|Early Photos of Y...| 2942958|\n",
      "|  366|Investment Firm Y...| 3492711|\n",
      "|  344|Y Combinator anno...| 1898432|\n",
      "|  342|Offer HN now at n...| 1840060|\n",
      "|  337|A note I sent to ...| 4273460|\n",
      "|  336|I am Sam Altman, ...|10360911|\n",
      "+-----+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YC_df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories of major companies over the year\n",
    "# apple\n",
    "sqlStatement = \"\"\"\n",
    "SELECT count(*) as total_stories, SUM(score) as total_score,\n",
    "year(time_ts) as year\n",
    "FROM story\n",
    "WHERE title like \"%apple%\" or title like \"%APPLE\" or title like \"%Apple\"\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "Comp_df1 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----+\n",
      "|total_stories|total_score|year|\n",
      "+-------------+-----------+----+\n",
      "|            1|          7|2006|\n",
      "|           28|        109|2007|\n",
      "|           43|        324|2008|\n",
      "|           86|        780|2009|\n",
      "|          194|       1964|2010|\n",
      "|          348|       5316|2011|\n",
      "|          382|       4883|2012|\n",
      "|          280|       2345|2013|\n",
      "|          203|       2302|2014|\n",
      "|          168|       3672|2015|\n",
      "+-------------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Comp_df1.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories of major companies over the year\n",
    "# google\n",
    "sqlStatement = \"\"\"\n",
    "SELECT count(*) as total_stories, SUM(score) as total_score,\n",
    "year(time_ts) as year\n",
    "FROM story\n",
    "WHERE title like \"%google%\" or title like \"%GOOGLE\" or title like \"%Google\"\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "Comp_df2 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----+\n",
      "|total_stories|total_score|year|\n",
      "+-------------+-----------+----+\n",
      "|            1|          1|2006|\n",
      "|          102|        514|2007|\n",
      "|          190|       1192|2008|\n",
      "|          428|       3376|2009|\n",
      "|          553|       6938|2010|\n",
      "|          506|       9489|2011|\n",
      "|          461|       7592|2012|\n",
      "|          478|      10844|2013|\n",
      "|          432|       7835|2014|\n",
      "|          369|       9247|2015|\n",
      "+-------------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Comp_df2.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories of major companies over the year\n",
    "# uber\n",
    "sqlStatement = \"\"\"\n",
    "SELECT count(*) as total_stories, SUM(score) as total_score,\n",
    "year(time_ts) as year\n",
    "FROM story\n",
    "WHERE title like \"%uber%\" or title like \"%UBER\" or title like \"%Uber\"\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "Comp_df3 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----+\n",
      "|total_stories|total_score|year|\n",
      "+-------------+-----------+----+\n",
      "|            3|          3|2007|\n",
      "|           22|        202|2008|\n",
      "|           23|        957|2009|\n",
      "|           59|       1110|2010|\n",
      "|           69|        966|2011|\n",
      "|           58|        874|2012|\n",
      "|           67|        523|2013|\n",
      "|          244|       2743|2014|\n",
      "|          302|       3301|2015|\n",
      "+-------------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Comp_df3.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset comments\n",
    "data_df2 = spark.read\\\n",
    "  .format('csv')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('s3://chingsez/Final/comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+----------+--------------------+--------------------+--------------------+--------------------+-------+-------+\n",
      "|                  id|     by|author|      time|             time_ts|                text|              parent|             deleted|   dead|ranking|\n",
      "+--------------------+-------+------+----------+--------------------+--------------------+--------------------+--------------------+-------+-------+\n",
      "|             2701393|     5l|    5l|1309184881|2011-06-27 14:28:...|And the glazier w...|             2701243|                null|   null|      0|\n",
      "|             5811403|     99|    99|1370234048|2013-06-03 04:34:...|Does canada have ...|             5804452|                null|   null|      0|\n",
      "|               21623|     AF|    AF|1178992400|2007-05-12 17:53:...|\"Speaking of Rail...|               21611|                null|   null|      0|\n",
      "|            10159727|     EA|    EA|1441206574|2015-09-02 15:09:...|Humans and large ...|            10159396|                null|   null|      0|\n",
      "|             2988424|     Iv|    Iv|1315853580|2011-09-12 18:53:...|I must say I reac...|             2988179|                null|   null|      0|\n",
      "|             3867418|     Iv|    Iv|1334921984|2012-04-20 11:39:...|&#62; There's a w...|             3867404|                null|   null|      0|\n",
      "|             3925617|     Iv|    Iv|1336076765|2012-05-03 20:26:...|I'm also in this ...|             3924840|                null|   null|      0|\n",
      "|             3107534|     Iv|    Iv|1318520044|2011-10-13 15:34:...|how do you run un...|                null|                null|   null|   null|\n",
      "|It has always ref...|3107241|  null|      null|                   0|                null|                null|                null|   null|   null|\n",
      "|             8409259|     Iv|    Iv|1412421647|2014-10-04 11:20:...|Polio is not exte...|             8409226|                null|   null|      0|\n",
      "|             2855741|     Jd|    Jd|1312690646|2011-08-07 04:17:...|\"Yep, I didn't fi...| but nothing that...| it may not be th...|2855343|   null|\n",
      "|               50570|     Jd|    Jd|1189011845|2007-09-05 17:04:...|It was a risky jo...|               50556|                null|   null|      0|\n",
      "|             2600618|     Jd|    Jd|1306794854|2011-05-30 22:34:...|\"Looks good, ther...|             2600609|                null|   null|      0|\n",
      "|             2600423|     Jd|    Jd|1306789205|2011-05-30 21:00:...|A bit, but so muc...|             2599323|                null|   null|      0|\n",
      "|             1983932|     Jd|    Jd|1291831945|2010-12-08 18:12:...|I also agree with...|             1979965|                null|   null|      0|\n",
      "|             5824036|     Jd|    Jd|1370414140|2013-06-05 06:35:...|Sadly doesn't pro...|             5824021|                null|   null|      0|\n",
      "|               73111|     Jd|    Jd|1193467001|2007-10-27 06:36:...|Feferman usefully...|               73107|                null|   null|      0|\n",
      "|             4569290|     Jd|    Jd|1348562302|2012-09-25 08:38:...|\"Here are my take...|             4569255|                null|   null|      0|\n",
      "|              319968|     KB|    KB|1222805047|2008-09-30 20:04:...|\"You may find a f...|              319943|                null|   null|      0|\n",
      "|             9699296|     M8|    M8|1434026580|2015-06-11 12:43:...|       For instance?|             9698927|                null|   null|      0|\n",
      "+--------------------+-------+------+----------+--------------------+--------------------+--------------------+--------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- by: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- time_ts: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- parent: string (nullable = true)\n",
      " |-- deleted: string (nullable = true)\n",
      " |-- dead: string (nullable = true)\n",
      " |-- ranking: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9796725"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "comment_df = data_df2.select(\n",
    "                     data_df2['id'].cast(IntegerType()),                    \n",
    "                     data_df2['time_ts'].cast('timestamp'),                  \n",
    "                     data_df2['text'],\n",
    "                     data_df2['parent'].cast(IntegerType()),\n",
    "                     #data_df2['deleted'],\n",
    "                     #data_df2['dead'],\n",
    "                     data_df2['author'],\n",
    "                     data_df2['ranking'].cast(IntegerType())\n",
    "\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- time_ts: timestamp (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- parent: integer (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- ranking: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+--------+------+-------+\n",
      "|      id|            time_ts|                text|  parent|author|ranking|\n",
      "+--------+-------------------+--------------------+--------+------+-------+\n",
      "| 2701393|2011-06-27 14:28:01|And the glazier w...| 2701243|    5l|      0|\n",
      "| 5811403|2013-06-03 04:34:08|Does canada have ...| 5804452|    99|      0|\n",
      "|   21623|2007-05-12 17:53:20|\"Speaking of Rail...|   21611|    AF|      0|\n",
      "|10159727|2015-09-02 15:09:34|Humans and large ...|10159396|    EA|      0|\n",
      "| 2988424|2011-09-12 18:53:00|I must say I reac...| 2988179|    Iv|      0|\n",
      "| 3867418|2012-04-20 11:39:44|&#62; There's a w...| 3867404|    Iv|      0|\n",
      "| 3925617|2012-05-03 20:26:05|I'm also in this ...| 3924840|    Iv|      0|\n",
      "| 3107534|2011-10-13 15:34:04|how do you run un...|    null|    Iv|   null|\n",
      "|    null|               null|                null|    null|  null|   null|\n",
      "| 8409259|2014-10-04 11:20:47|Polio is not exte...| 8409226|    Iv|      0|\n",
      "+--------+-------------------+--------------------+--------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df.createOrReplaceTempView('ini_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8399564"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df.select(\"id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_comment_df = spark.sql('select * from ini_comment where id is not NULL and time_ts is not NULL and text is not NULL and author is not NULL and ranking is not NULL')\n",
    "clean_comment_df.createOrReplaceTempView('comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6986995"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_comment_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------------------+--------+------+-------+\n",
      "|      id|            time_ts|                text|  parent|author|ranking|\n",
      "+--------+-------------------+--------------------+--------+------+-------+\n",
      "| 2701393|2011-06-27 14:28:01|And the glazier w...| 2701243|    5l|      0|\n",
      "| 5811403|2013-06-03 04:34:08|Does canada have ...| 5804452|    99|      0|\n",
      "|   21623|2007-05-12 17:53:20|\"Speaking of Rail...|   21611|    AF|      0|\n",
      "|10159727|2015-09-02 15:09:34|Humans and large ...|10159396|    EA|      0|\n",
      "| 2988424|2011-09-12 18:53:00|I must say I reac...| 2988179|    Iv|      0|\n",
      "| 3867418|2012-04-20 11:39:44|&#62; There's a w...| 3867404|    Iv|      0|\n",
      "| 3925617|2012-05-03 20:26:05|I'm also in this ...| 3924840|    Iv|      0|\n",
      "| 8409259|2014-10-04 11:20:47|Polio is not exte...| 8409226|    Iv|      0|\n",
      "|   50570|2007-09-05 17:04:05|It was a risky jo...|   50556|    Jd|      0|\n",
      "| 2600618|2011-05-30 22:34:14|\"Looks good, ther...| 2600609|    Jd|      0|\n",
      "+--------+-------------------+--------------------+--------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_comment_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment author/user information \n",
    "# who are the most contributive authors in Hacker News ?\n",
    "## futhur to be done——can be joined with the most contirbutive authors in story\n",
    "\n",
    "sqlStatement = \"\"\"SELECT author,\n",
    "        COUNT(id) AS total_comments\n",
    "        From comment\n",
    "        GROUP BY author\n",
    "        ORDER BY total_comments DESC\n",
    "        LIMIT 20\n",
    "\"\"\"\n",
    "comment_user_df = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|      author|total_comments|\n",
      "+------------+--------------+\n",
      "|     tptacek|         28605|\n",
      "|    jacquesm|         19845|\n",
      "|       DanBC|         10992|\n",
      "|    jrockway|         10679|\n",
      "|   anigbrowl|         10490|\n",
      "|dragonwriter|         10203|\n",
      "|         eru|          9979|\n",
      "|     rbanffy|          9634|\n",
      "|       sp332|          9547|\n",
      "|     rayiner|          9403|\n",
      "+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show top 10\n",
    "comment_user_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment distribution in terms of time\n",
    "# What is the trend of comment numbers from 2006-2016 in Hacker News?\n",
    "\n",
    "sqlStatement = \"\"\"\n",
    "SELECT COUNT(id) AS comment_num, \n",
    "year(time_ts) as year\n",
    "FROM comment\n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "\"\"\"\n",
    "comment_time_df2 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|comment_num|year|\n",
      "+-----------+----+\n",
      "|         10|2006|\n",
      "|      50982|2007|\n",
      "|     195678|2008|\n",
      "|     382490|2009|\n",
      "|     658846|2010|\n",
      "|     816999|2011|\n",
      "|     975306|2012|\n",
      "|    1415381|2013|\n",
      "|    1348324|2014|\n",
      "|    1142979|2015|\n",
      "+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment_time_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment distribution in terms of time\n",
    "# What is the most popular/active time during a day in pubulishing a comment?\n",
    "\n",
    "sqlStatement = \"\"\"\n",
    "SELECT COUNT(id) AS comment_num, \n",
    "hour(time_ts) as hour\n",
    "FROM comment\n",
    "GROUP BY hour\n",
    "ORDER BY comment_num DESC\n",
    "\"\"\"\n",
    "comment_time_df = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|comment_num|hour|\n",
      "+-----------+----+\n",
      "|     451259|  17|\n",
      "|     450094|  18|\n",
      "|     437369|  16|\n",
      "|     433640|  19|\n",
      "|     421504|  20|\n",
      "|     413518|  15|\n",
      "|     398429|  21|\n",
      "|     365798|  14|\n",
      "|     353708|  22|\n",
      "|     313262|  23|\n",
      "|     295136|  13|\n",
      "|     276703|   0|\n",
      "|     251256|   1|\n",
      "|     236036|   2|\n",
      "|     224956|   3|\n",
      "|     224367|  12|\n",
      "|     210740|   4|\n",
      "|     192913|   5|\n",
      "|     180795|   6|\n",
      "|     180141|  11|\n",
      "|     172147|   7|\n",
      "|     170884|   8|\n",
      "|     168104|   9|\n",
      "|     164236|  10|\n",
      "+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment_time_df.show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top(the hottest) comments with the most follow-up comment \n",
    "sqlStatement = \"\"\"\n",
    "SELECT COUNT(id) AS followup_num, \n",
    "parent\n",
    "FROM comment\n",
    "WHERE parent is not NULL\n",
    "GROUP BY parent\n",
    "ORDER BY followup_num DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "comment_parent_df1 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|followup_num|  parent|\n",
      "+------------+--------+\n",
      "|         975|     363|\n",
      "|         266| 7469115|\n",
      "|         266| 9996333|\n",
      "|         264| 9238839|\n",
      "|         262| 9812245|\n",
      "|         243| 7445761|\n",
      "|         241|10152809|\n",
      "|         239|  752262|\n",
      "|         234| 9471287|\n",
      "|         228| 9303396|\n",
      "+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show top 10\n",
    "comment_parent_df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(ranking)=-1051, max(ranking)=1131019295, avg(ranking)=58481.1891668736)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ranking information\n",
    "# what is the average ranking of those comments?\n",
    "\n",
    "spark.sql(\"select min(ranking),max(ranking),avg(ranking) from comment\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author/user's ranking information decending by avg ranking \n",
    "# who has the highest avg ranking?\n",
    "\n",
    "sqlStatement = \"\"\"SELECT avg(ranking) as avg_ranking,author,\n",
    "count(id) as total_comment\n",
    "FROM comment\n",
    "GROUP BY author\n",
    "ORDER BY avg(ranking) DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "comment_user_df1 = spark.sql(sqlStatement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------+\n",
      "|         avg_ranking|      author|total_comment|\n",
      "+--------------------+------------+-------------+\n",
      "|         2.0630034E7|AretNCarlsen|           56|\n",
      "|1.5151532969696969E7|     dedalus|           66|\n",
      "|         1.0360911E7|      pratim|            1|\n",
      "|         1.0359641E7|    vmuhonen|            1|\n",
      "|           1.03454E7|   scopesoft|            1|\n",
      "|         1.0288613E7|  FlexMonkey|            1|\n",
      "|         1.0277638E7|   Jimbobian|            1|\n",
      "|         1.0275866E7| waynebeaton|            1|\n",
      "|         1.0250132E7| anonttttttt|            1|\n",
      "|         1.0230206E7|      bduhan|            1|\n",
      "+--------------------+------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show top 10\n",
    "comment_user_df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
